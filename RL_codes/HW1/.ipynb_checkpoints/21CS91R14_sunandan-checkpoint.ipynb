{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKQquN4dlFu8"
   },
   "source": [
    "# **CS60077: Reinforcement Learning**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Homework 01**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Y3nwwyleVv"
   },
   "source": [
    "---\n",
    "**Student Details:** (Make sure to fill your details before submitting, \n",
    "\n",
    "---\n",
    "**Name:** \n",
    "\n",
    "```\n",
    "<Sunandan Adhikary>\n",
    "```\n",
    "\n",
    "**Roll Number:** \n",
    "\n",
    "```\n",
    "<21CS91R14>\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4L26uv8Qs2b"
   },
   "source": [
    "The Assignment Consists of Two parts. The first part consists of a couple of descriptive questions. These questions are written in the notebook itself with space provided to add your answer. It is encouraged to type your answers in the notebook itself utilising the inbuilt $\\LaTeX$ commands. But you may write these answers on a sheet of paper, scan it and upload it with this assignment. The second part consists of a coding task where you need to implement the Dynamic Programming algorithms you learnt in class (Policy Evaluation, Policy Iteration and Value Iteration) on a simple RL environment called ***Frozen Lake***. We have written some portions of the code and you only need to fill in your implementation of the algorithms in the space provided. \n",
    "\n",
    "**Submission Instruction:** Submit the completed `.ipynb` notebook and a pdf containing the solutions to the descriptive questions (only if you haven't written it in the notebook) to moodle. Naming convention: `<RollNumber>_<FirstName>.ipynb`. \n",
    "\n",
    "*We encourage students NOT to upload any other file. Uploading unnecessary files may lead to penalties.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqgh7eJORzRe"
   },
   "source": [
    "---\n",
    "---\n",
    "`**DESCRIPTIVE TASK**` Answer the following questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpcvNCUMTmPO"
   },
   "source": [
    "[**Question 1**]: Let $v(s)$ and $q(s,a)$ denote the state and action value functions respectively for an MDP where $s\\in\\mathcal{S}$ denotes a state and $a\\in\\mathcal{A}$ denotes an action. $\\mathcal{S}$ and $\\mathcal{A}$ are the state space and action space respectively. Assume $\\mathbf{v}$ and $\\mathbf{q}$ are vectors of lengths $|\\mathcal{S}|$ and $|\\mathcal{S}|\\cdot|\\mathcal{A}|$ respectively [Note that you can think of $\\mathbf{q}$ as a vector where the number of elements in such a vector is $|\\mathcal{S}|\\cdot|\\mathcal{A}|$]. Let $\\tilde{\\mathbf{q}}$ is any vector in $\\mathbb{R}^{|\\mathcal{S}|\\cdot|\\mathcal{A}|}$ denoting a legitimate action value function and $\\mathbf{q}^*$ is the optimal value function for the MDP. Let $\\pi$ be the greedy policy w.r.t.  $\\tilde{\\mathbf{q}}$. Then prove the following.\n",
    "\\begin{equation}\n",
    "\tv^*(s) - v_\\pi(s) \\leq 2||\\mathbf{q}^* - \\tilde{\\mathbf{q}}||_\\infty + \\gamma ||\\mathbf{v}^* - \\mathbf{v}_\\pi||_\\infty\n",
    "\\end{equation}\n",
    "where, $\\gamma$ is the discount factor, $v_\\pi(s)$ is the state-value function corresponding to the greedy policy $\\pi$ and $v^*(s)$ is the optimal value function. Similarly, $\\mathbf{v}_\\pi$ and $\\mathbf{v}^*$ are the vector of all value functions for policy $\\pi$ and the optimal policy respectively.\n",
    "\n",
    "**For your convenience the following starter is provided**:\n",
    "\n",
    "\\begin{align}\n",
    "v^*(s) - v_\\pi(s)  &= v^*(s) - q^*(s,\\pi(s)) + q^*(s,\\pi(s)) - v_\\pi(s) \\\\\n",
    "&= v^*(s) - q^*(s,\\pi(s)) + q^*(s,\\pi(s)) - \\tilde{q}(s,\\pi(s)) \\quad [\\text{ since } v_\\pi(s)=\\tilde{q}(s,\\pi(s))] \\\\\n",
    "&= v^*(s) - q^*(s,\\pi(s)) \\cdots \\text{use Bellman eqn. for the last two terms above and relate the }\\\\\n",
    "& \\text{resulting expression to the infinity norm of the difference between } \\mathbf{v}^* \\text{ and } \\mathbf{v}_\\pi\n",
    "\\end{align}\n",
    "\n",
    "Now try to manipulate the first two terms of the above, as follows.\n",
    "\\begin{align}\n",
    "v^*(s) - q^*(s,\\pi(s))  &= v^*(s) - \\tilde{q}(s,\\pi(s)) + \\tilde{q}(s,\\pi(s)) - q^*(s,\\pi(s)) \\\\\n",
    "&= q^*(s,\\pi^*(s)) - \\tilde{q}(s,\\pi(s)) + \\tilde{q}(s,\\pi(s)) - q^*(s,\\pi(s))\\\\\n",
    "&\\text{For the last two terms use the fact that an element of a vector is less than}\\\\\n",
    "&\\text{the infinity norm of the vector.}\\\\\n",
    "&\\text{For the first two terms also you have to use a similar trick, but for that }\\\\\n",
    "&\\tilde{q}(s,\\pi(s)) \\text{ needs to be replaced with } \\tilde{q}(s,\\pi^*(s))\\\\\n",
    "&\\text{by properly using the inequality between the two}.\n",
    "\t\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oTL2ExRTqKE"
   },
   "source": [
    "#### Answer 1 :\n",
    "\n",
    "As hinted, we can write, \n",
    "\\begin{align}\n",
    "v^*(s) - v_\\pi(s)  &= v^*(s) - q^*(s,\\pi(s)) + q^*(s,\\pi(s)) - v_\\pi(s) \\\\\n",
    "                   &= v^*(s) - q^*(s,\\pi(s)) + q^*(s,\\pi(s)) - \\tilde{q}(s,\\pi(s)) \\quad [\\text{ since } v_\\pi(s)=\\tilde{q}(s,\\pi(s))] \\\\     &= \\mathbf{(i)} + \\mathbf{(ii)} \\quad \\text{where, } [v^*(s) - q^*(s,\\pi(s))] \\text{ is }\\mathbf{(i)} \\text{ and  } [q^*(s,\\pi(s)) - \\tilde{q}(s,\\pi(s))] \\text{ is } \\mathbf{(ii)} \n",
    "\\end{align}\n",
    "For the second part we can write, \\begin{align}\n",
    "\\mathbf{(ii)}\\ q^*(s,\\pi(s)) - \\tilde{q}(s,\\pi(s)) &\\leq q^*(s,\\pi^*(s)) - \\tilde{q}(s,\\pi(s))\\ [\\text{since by definition of optimal policy, }q^*(s,\\pi^*(s)) \\geq q^*(s,\\pi(s))]\\\\ \n",
    "                                                   &\\leq \\Vert q^* - \\tilde{q} \\Vert_{\\infty}\\ [\\text{shortening the terms}]\n",
    "\\end{align}\n",
    "For the first part we can write as hinted,\n",
    "\\begin{align}\n",
    "\\mathbf{(i)}\\ v^*(s) - q^*(s,\\pi(s))  &= v^*(s) - \\tilde{q}(s,\\pi(s)) + \\tilde{q}(s,\\pi(s)) - q^*(s,\\pi(s)) \\\\\n",
    "                                      &\\leq v^*(s) - \\tilde{q}(s,\\pi(s)) + \\Vert q^* - \\tilde{q} \\Vert_{\\infty}\\\\\n",
    "                                      &\\leq q^*(s,\\pi^*(s)) - \\tilde{q}(s,\\pi(s)) + \\Vert q^* - \\tilde{q} \\Vert_{\\infty}\\\\\n",
    "                                      &\\leq r(s, \\pi^*(s)) + \\gamma \\mathcal{P}(s'|s)*q^*(s',\\pi^*(s')) - r(s, \\pi^*(s)) - \\gamma \\mathcal{P}(s'|s)*\\tilde{q}(s,\\pi(s)) + \\Vert q^* - \\tilde{q} \\Vert_{\\infty}\\ [\\text{from bellman's optimality and state-value equations}]\\\\\n",
    "                                      &\\leq \\gamma \\mathcal{P}(s'|s)*q^*(s',\\pi^*(s')) - \\gamma \\mathcal{P}(s'|s)*\\tilde{q}(s,\\pi(s)) + \\Vert q^* - \\tilde{q} \\Vert_{\\infty} \\\\\n",
    "                                      &\\leq \\gamma \\mathcal{P}(s'|s) \\Vert v^*(s')) - v_{\\pi}(s',\\pi(s))\\Vert_{\\infty} + \\Vert q^* - \\tilde{q} \\Vert_{\\infty}\\ [\\text{using triangle inequality}]\\\\\n",
    "                                      &\\leq \\gamma \\Vert v^* - v_{\\pi}\\Vert_{\\infty} + \\Vert q^* - \\tilde{q} \\Vert_{\\infty}\\ [\\text{since $\\mathcal{P}$ is a stochastic matrix, hence has eigen values $\\leq 1$}] \n",
    "\\end{align}\n",
    "Hence by summing up $\\mathbf{(i)}$ and $\\mathbf{(ii)}$ we get, \n",
    "\\begin{align}\n",
    "v^*(s) - v_\\pi(s)   &\\leq \\gamma \\Vert v^* - v_{\\pi} \\Vert_{\\infty} + 2\\Vert q^* - \\tilde{q} \\Vert_{\\infty} \\quad [\\textbf{Proved}]   \n",
    "\\end{align} \n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Question 2**]: You have already proved the policy evaluation for the modifed Bellman Operator for action value function in your Quiz 2. To remind you, the modifed Bellman Operator was defined as:\n",
    "\\begin{equation}\n",
    "T^{\\pi} Q(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}_{s_{t+1} \\sim p}[V(s_{t+1})]\n",
    "\\end{equation}\n",
    "where the modified value function $V(s_t)$ is defined as, \n",
    "\\begin{equation}\n",
    "V(s_t) = \\mathbb{E}_{a_t \\sim \\pi}[Q(s_t, a_t) - \\text{log} \\pi(a_t|s_t)]\n",
    "\\end{equation}\n",
    "\n",
    "Now consider to policy update operation defined as, \n",
    "\\begin{equation}\n",
    "\\pi_{\\text{new}} = \\text{argmin}_{\\pi^{'} \\in \\Pi}D_{KL} \\Big( \\pi^{'}(\\cdot | s_t) \\Big| \\Big| \\frac{\\text{exp}(Q^{\\pi_{\\text{old}}}(s_t, \\cdot))}{Z^{\\pi_{\\text{old}}}(s_t)} \\Big)\n",
    "\\end{equation}\n",
    "where $\\Pi$ is the set of all policies, $Z^{\\pi^{\\text{old}}}(s_t)$ is the partition function used to normalise the distribution and $D_{KL}$ is the Kullback-Leiblar Divergence. The KL-Divergence between two distributions $p$ and $q$ is defined as, \n",
    "\\begin{equation}\n",
    "D_{KL}(p || q) = \\mathbb{E}_{p}\\Big[\\text{log}\\frac{p}{q} \\Big] = \\int p(x) \\text{log}\\frac{p(x)}{q(x)}dx\n",
    "\\end{equation}\n",
    "\n",
    "You need to prove that $Q^{\\pi_{\\text{new}}}(s_t, a_t) \\ge Q^{\\pi_{\\text{old}}}(s_t, a_t)$ for all $(s_t, a_t) \\in S \\times A$. Assume $|A| < \\infty$.\n",
    "\n",
    "***Hint***: Define the function $J$ as, \n",
    "\\begin{equation}\n",
    " J^{\\pi_{\\text{old}}}(\\pi) = D_{KL} \\Big( \\pi(\\cdot | s_t) \\Big| \\Big| \\frac{\\text{exp}(Q^{\\pi_{\\text{old}}}(s_t, \\cdot))}{Z^{\\pi_{\\text{old}}}(s_t)} \\Big)\n",
    " \\end{equation}\n",
    "\n",
    "You should agree with the fact the $J^{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) \\le J^{\\pi_{\\text{old}}}(\\pi_{\\text{old}})$. Now simply expand $J$ using the definition of KL-Divergence and simplify to get an upper bound on $V^{\\pi_{\\text{old}}}(s_t)$. Using this bound and the modified Bellman Operator, repeatedly expand $Q^{\\pi_{\\text{old}}}$ and iteratively apply policy evaluation operation i.e. $Q^{k+1} = T^{\\pi}Q^{k}$ to prove $Q^{\\pi_{\\text{new}}}(s_t, a_t) \\ge Q^{\\pi_{\\text{old}}}(s_t, a_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOT6P9Djb0P2"
   },
   "source": [
    "#### Answer 2 :\n",
    "As hinted if we consider $\\pi_{new} = \\arg\\min\\limits_{\\pi\\in \\Pi}J^{\\pi_{old}}(\\pi)$ then $J^{\\pi_{old}}(\\pi_{new}) \\leq J^{\\pi_{old}}(\\pi_{old})$ is always true. Since, \\begin{equation}\n",
    "J^{\\pi_{old}}(\\pi_{new}) = D_{KL} \\Big( \\pi_{new}(\\cdot | s_t) \\Big| \\Big| \\frac{\\text{exp}(Q^{\\pi_{\\text{old}}}(s_t, \\cdot))}{Z^{\\pi_{\\text{old}}}(s_t)} \\Big) = \\mathbb{E}\\limits_{a_t \\sim \\pi_{new}}\\Big[ \\text{log}{\\frac{\\pi_{new}(a_t | s_t)\\times Z^{\\pi_{\\text{old}}}(s_t)}{\\text{exp}(Q^{\\pi_{\\text{old}}}}}\\Big]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaWOoclVihnQ"
   },
   "source": [
    "---\n",
    "---\n",
    "`**CODING TASK**` You need to implement value iteration and policy iteration for the Frozen Lake environment from [OpenAI Gym](https://gym.openai.com/envs/FrozenLake-v0). We have provided custom versions of this environment in the starter code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXxadMSJjNCo"
   },
   "source": [
    "**How to Start?**\n",
    "\n",
    "*   Upload the shared zip file (`CS60077_HW1_StarterCodes.zip`) to Colab and you are good to go for executing the cells below!\n",
    "\n",
    "\n",
    "*Please do NOT change the structure of the code. We expect you to write your codes in the desired segment mentioned with comments ONLY.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZunEUyskFAA"
   },
   "source": [
    "Unzip the uploaded file by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7WrlcP-kEpt"
   },
   "outputs": [],
   "source": [
    "# !unzip CS60077_HW1_StarterCodes.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lF1rGIeakV4r"
   },
   "source": [
    "Install all the necessary dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YGGtbKFhATW"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voNMcbcXkab9"
   },
   "source": [
    "Import the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGAQ3iA6fjTK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *\n",
    "\n",
    "np.set_printoptions(precision=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-F4FlIHgADl"
   },
   "source": [
    "For the functions (defined below) `policy_evaluation`, `policy_improvement`, `policy_iteration` and `value_iteration`,\n",
    "the parameters `P`, `nS`, `nA`, `gamma` are defined as follows:\n",
    "\n",
    "\tP: nested dictionary\n",
    "\t\tFrom gym.core.Environment\n",
    "\t\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
    "\t\ttuple of the form (probability, nextstate, reward, terminal) where\n",
    "\t\t\t- probability: float\n",
    "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "\t\t\t- nextstate: int\n",
    "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
    "\t\t\t- reward: int\n",
    "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
    "\t\t\t\t\"nextstate\" with \"action\"\n",
    "\t\t\t- terminal: bool\n",
    "\t\t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "\tnS: int\n",
    "\t\tnumber of states in the environment\n",
    "\tnA: int\n",
    "\t\tnumber of actions in the environment\n",
    "\tgamma: float\n",
    "\t\tDiscount factor. Number in range [0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXn0qb6_f74r"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "\t\"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\tpolicy: np.array[nS]\n",
    "\t\tThe policy to evaluate. Maps states to actions.\n",
    "\ttol: float\n",
    "\t\tTerminate policy evaluation when\n",
    "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
    "\tReturns\n",
    "\t-------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\t\tThe value function of the given policy, where value_function[s] is\n",
    "\t\tthe value of state s\n",
    "\t\"\"\"\n",
    "    \n",
    "\tvalue_function = np.zeros(nS)\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\tdiff = -1\n",
    "\tprint(\"---- Evaluating policy-----\")\n",
    "\twhile not diff < tol or diff == -1:\n",
    "\t\tdiff = 0\n",
    "\t\tfor s in range(nS):\n",
    "\t\t\tv = value_function[s]\n",
    "\t\t\tvalue_function[s] = 0\n",
    "\t\t\ta = policy[s]\n",
    "\t\t\t# print(\"old state value = \"+str(v))\n",
    "\t\t\tfor (prob, nxst, rwd, term) in P[s][a]:\n",
    "\t\t\t\t# value_function[s] = rwd\n",
    "\t\t\t\tif ~term :\n",
    "\t\t\t\t\tvalue_function[s] += prob*(rwd + gamma*value_function[nxst])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tvalue_function[s] += prob*rwd\n",
    "\t\t\t\t# print(\"new state value \"+ str(value_function[s])+\" with action \"+str(a))\n",
    "\t\t\tdiff = max(diff, abs(v - value_function[s]))\n",
    "\t\t\t# print(\"For current policy evaluating V(\"+str(s)+\") = \"+str(value_function[s])+\" with difference = \"+str(diff))  \n",
    "\t############################\n",
    "\treturn value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvuLPbTpgTNM"
   },
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "\t\"\"\"Given the value function from policy improve the policy.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\tvalue_from_policy: np.ndarray\n",
    "\t\tThe value calculated from the policy\n",
    "\tpolicy: np.array\n",
    "\t\tThe previous policy.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tnew_policy: np.ndarray[nS]\n",
    "\t\tAn array of integers. Each integer is the optimal action to take\n",
    "\t\tin that state according to the environment dynamics and the\n",
    "\t\tgiven value function.\n",
    "\t\"\"\"\n",
    "\tnew_policy = np.zeros(nS, dtype='int')\n",
    "\tvalue_function = value_from_policy\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\tfor s in range(nS):\n",
    "\t\tnew_policy[s] = policy[s]\n",
    "\t\t# print(\"For \"+str(s)+\"-th state available actions : ---\")\n",
    "\t\tfor a in range(nA):\n",
    "\t\t\tq = 0\n",
    "\t\t\tfor (prob, nxst, rwd, term) in P[s][a]:\n",
    "\t\t\t\tif ~term :\n",
    "\t\t\t\t\tq += prob*(rwd + gamma*value_function[nxst])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tq += prob*rwd\n",
    "\t\t\t# print(\"Action Value for choice \"+str(P[s][a])+\" q = \"+str(q))\n",
    "\t\t\tif q > value_function[s]:\n",
    "\t\t\t\t# print(\"Since for (state,action) = (\"+str(s)+\",\"+str(a)+\") the state-action value q = \"+str(q)+\" is greater than state value \"+str(value_function[s]))\n",
    "\t\t\t\tvalue_function[s] = q\n",
    "\t\t\t\tnew_policy[s] = a\n",
    "\t\t\t\t# print(\"......Therefore choosing Action \"+str(new_policy[s])+\" from old policy \"+str(policy[s]))\n",
    "\t\t# print(\"Improving policy for state \"+str(s)+\" from action \"+str(policy[s])+\" by choosing (action,value) = (\"+str(new_policy[s])+\",\"+str(value_function[s])+\")\") \n",
    "\tprint(\"old policy : \"+str(policy)+\" improved to new policy: \"+str(new_policy))\n",
    "\t############################\n",
    "\treturn new_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-Z8abzugVoZ"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
    "\t\"\"\"Runs policy iteration.\n",
    "\n",
    "\tYou should call the policy_evaluation() and policy_improvement() methods to\n",
    "\timplement this method.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\ttol: float\n",
    "\t\ttol parameter used in policy_evaluation()\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\tpolicy: np.ndarray[nS]\n",
    "\t\"\"\"\n",
    "\tvalue_function = np.zeros(nS)\n",
    "\tpolicy = np.zeros(nS, dtype=int)\n",
    "\t\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\tpolicy_stable = 0\n",
    "\twhile  not policy_stable:\n",
    "\t\t\n",
    "\t\tvalue_from_policy = policy_evaluation(P, nS, nA, policy)\n",
    "\t\t# print (\"-\"*10+\"starting policy improvement with old policy \"+str(policy)+\"-\"*10)\n",
    "\t\tnew_policy = policy_improvement(P, nS, nA, value_from_policy, policy)\n",
    "\t\tdiff = abs(new_policy-policy)\t\n",
    "\t\tif not np.max(abs(diff)) == 0 :\n",
    "\t\t\tprint(\"policy yet to stabilize\")\n",
    "\t\t\tpolicy = new_policy\n",
    "\t\telse:\n",
    "\t\t\tpolicy_stable = 1\n",
    "\t\t\tvalue_function = value_from_policy\n",
    "\t\t\tprint(\"Final policy: \"+str(policy)+\" with state values = \"+str(value_function))\n",
    "\n",
    "\t############################\n",
    "\treturn value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaRLOIG2gaNq"
   },
   "outputs": [],
   "source": [
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "\t\"\"\"\n",
    "\tLearn value function and policy by using value iteration method for a given\n",
    "\tgamma and environment.\n",
    "\n",
    "\tParameters:\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\ttol: float\n",
    "\t\tTerminate value iteration when\n",
    "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\tpolicy: np.ndarray[nS]\n",
    "\t\"\"\"\n",
    "\tvalue_function = np.zeros(nS)\n",
    "\tpolicy = np.zeros(nS, dtype=int)\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\tdiff = -1\n",
    "\twhile not diff <= tol*(1-gamma)/(2*gamma) or diff == -1:\n",
    "\t\tdiff = 0\n",
    "\t\tfor s in range(nS):\n",
    "\t\t\tq = 0\n",
    "\t\t\told_v = value_function[s]\n",
    "\t\t\tvalue_function[s] = 0\n",
    "\t\t\tfor a in range(nA):\n",
    "\t\t\t\tfor (prob,nxst,rwd,term) in P[s][a]:\n",
    "\t\t\t\t\tif term:\n",
    "\t\t\t\t\t\tq += prob*rwd\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tq = prob*(rwd + gamma * value_function[nxst])\n",
    "\t\t\t\tif q > value_function[s] :\n",
    "\t\t\t\t\tvalue_function[s] = q\n",
    "\t\t\t\t\tpolicy[s] = a\n",
    "\t\t\tif diff < abs(old_v-value_function[s]):\n",
    "\t\t\t\tdiff = abs(old_v-value_function[s])\n",
    "\t\tprint(\"---Improving state values with policy \"+str(policy)+\"---\") \n",
    "\t############################\n",
    "\treturn value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KP-aKU3Bga0q"
   },
   "outputs": [],
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "  steps = max_steps\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      steps = t\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "    print(\"Number of Steps to reach goal: %f\" % steps)\n",
    "    print(\"Episode reward: %f\" % episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sagRbH33k4Su"
   },
   "source": [
    "Let's test now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7iCcCVKglE0"
   },
   "outputs": [],
   "source": [
    "# comment/uncomment these lines to switch between deterministic/stochastic environments\n",
    "# env = gym.make(\"Deterministic-8x8-FrozenLake-v0\")\n",
    "env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration \\n\" + \"-\"*25)\n",
    "\n",
    "V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env, p_pi, 80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration \\n\" + \"-\"*25)\n",
    "\n",
    "V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env, p_vi, 80)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS60077_HW1.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
